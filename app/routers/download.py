from fastapi import APIRouter, HTTPException
from fastapi.responses import FileResponse
from app.services.ml_training import MLTrainingService
from app.core.config import Settings
import zipfile
import io
import json
from pathlib import Path

router = APIRouter()
settings = Settings()
training_service = MLTrainingService(settings.models_dir)

@router.get("/model/{training_id}")
async def download_model(training_id: str):
    """Download trained model as a zip file"""
    try:
        # Check if model exists
        artifacts = training_service.load_model(training_id)
        
        model_dir = Path(settings.models_dir) / training_id
        
        # Create zip file in memory
        zip_buffer = io.BytesIO()
        
        with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
            # Add all files in the model directory
            for file_path in model_dir.rglob('*'):
                if file_path.is_file():
                    arcname = file_path.relative_to(model_dir)
                    zip_file.write(file_path, arcname)
            
            # Add a readme file with usage instructions
            readme_content = f"""
# AutoML Model Package - {training_id}

## Model Information
- Training ID: {training_id}
- Problem Type: {artifacts['metadata']['problem_type']}
- Best Model: {artifacts['metadata']['best_model_name']}

## Files Included
- best_model.joblib: The trained model
- metadata.json: Training metadata and performance metrics
- preprocessor.joblib: Data preprocessing pipeline (if applicable)
- target_encoder.joblib: Target variable encoder (if applicable)

## Usage Example

```python
import joblib
import pandas as pd

# Load the model
model = joblib.load('best_model.joblib')

# Load preprocessing pipeline (if exists)
try:
    preprocessor = joblib.load('preprocessor.joblib')
except:
    preprocessor = None

# Load target encoder (if exists)
try:
    target_encoder = joblib.load('target_encoder.joblib')
except:
    target_encoder = None

# Make predictions on new data
# new_data should be a pandas DataFrame with the same columns as training data
predictions = model.predict(new_data)

# If target encoder exists, inverse transform predictions
if target_encoder:
    predictions = target_encoder.inverse_transform(predictions)

print("Predictions:", predictions)
```

## Model Performance
See metadata.json for detailed performance metrics and training information.

Generated by AutoML Platform
"""
            zip_file.writestr("README.md", readme_content)
        
        zip_buffer.seek(0)
        
        # Save zip file temporarily
        temp_zip_path = model_dir / f"{training_id}_model.zip"
        with open(temp_zip_path, "wb") as f:
            f.write(zip_buffer.getvalue())
        
        return FileResponse(
            path=temp_zip_path,
            filename=f"automl_model_{training_id}.zip",
            media_type='application/zip'
        )
        
    except ValueError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error creating model package: {str(e)}")

@router.get("/report/{training_id}")
async def download_report(training_id: str):
    """Download training report as JSON"""
    try:
        artifacts = training_service.load_model(training_id)
        metadata = artifacts["metadata"]
        
        # Create comprehensive report
        report = {
            "training_summary": {
                "training_id": training_id,
                "problem_type": metadata["problem_type"],
                "best_model": metadata["best_model_name"],
                "training_timestamp": metadata.get("timestamp"),
                "feature_count": len(metadata.get("feature_names", [])),
                "models_trained": len(metadata.get("model_performances", []))
            },
            "model_performances": metadata.get("model_performances", []),
            "feature_names": metadata.get("feature_names", []),
            "data_info": {
                "original_columns": metadata.get("original_columns", {}),
                "preprocessing_applied": {
                    "numerical_scaling": "StandardScaler",
                    "categorical_encoding": "OneHotEncoder",
                    "missing_value_imputation": "Applied"
                }
            }
        }
        
        # Save report temporarily
        model_dir = Path(settings.models_dir) / training_id
        report_path = model_dir / "training_report.json"
        
        with open(report_path, "w") as f:
            json.dump(report, f, indent=2)
        
        return FileResponse(
            path=report_path,
            filename=f"automl_report_{training_id}.json",
            media_type='application/json'
        )
        
    except ValueError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error creating report: {str(e)}")

@router.get("/models")
async def list_models():
    """List all available trained models"""
    try:
        models_dir = Path(settings.models_dir)
        models = []
        
        if models_dir.exists():
            for model_dir in models_dir.iterdir():
                if model_dir.is_dir():
                    metadata_path = model_dir / "metadata.json"
                    if metadata_path.exists():
                        try:
                            with open(metadata_path, "r") as f:
                                metadata = json.load(f)
                            
                            models.append({
                                "training_id": model_dir.name,
                                "problem_type": metadata.get("problem_type"),
                                "best_model": metadata.get("best_model_name"),
                                "timestamp": metadata.get("timestamp"),
                                "feature_count": len(metadata.get("feature_names", []))
                            })
                        except:
                            continue  # Skip invalid model directories
        
        return {
            "models": sorted(models, key=lambda x: x.get("timestamp", 0), reverse=True),
            "total_models": len(models)
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error listing models: {str(e)}")